{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Process Input\n",
    "\n",
    "* Read input text.\n",
    "* Split intro training and validation.\n",
    "* Split each into chunks of `series_size` characters.\n",
    "* Batch up int random batches of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare writings: 4.36MB\n"
     ]
    }
   ],
   "source": [
    "encoding_depth = 128 # nr of chars in ascii\n",
    "\n",
    "with open(\"inputs/shakespeare.txt\") as file:\n",
    "    shakespeare = file.read().encode(\"ascii\")\n",
    "print(\"Shakespeare writings: {:.2f}MB\".format(len(shakespeare)/(1<<20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_frac = 0.95\n",
    "valid_idx = int(len(shakespeare)*train_frac)\n",
    "train, valid = shakespeare[:valid_idx], shakespeare[valid_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_io(chars):\n",
    "    chars = list(map(int, chars))\n",
    "    x = chars[:-1]\n",
    "    y = chars[1:] # predict next character\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "series_size = 64 # chunk of characters to process at a time\n",
    "def chunk_to_series(io):\n",
    "    x, y = io\n",
    "    return [\n",
    "        (x[i:i+series_size], y[i:i+series_size])\n",
    "        for i in range(0, len(x)-series_size, series_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "series = chunk_to_series(make_io(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transpose = lambda l: list(zip(*l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "def chunk_to_batches(series):\n",
    "    \"\"\"Return batches of randomly shuffled series.\"\"\"\n",
    "\n",
    "    batches_xy = [\n",
    "        series[i:i+batch_size]\n",
    "        \n",
    "        for i in range(0, len(series)-batch_size, batch_size)]\n",
    "    \n",
    "    return [transpose(batch_xy) for batch_xy in batches_xy]\n",
    "\n",
    "batches = chunk_to_batches(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def epoch(text):\n",
    "    return [transpose(chunk_to_series(make_io(text)))]\n",
    "\n",
    "def random_batched_epoch(text):\n",
    "    series = chunk_to_series(make_io(text))\n",
    "    \n",
    "    series = series.copy()\n",
    "    np.random.shuffle(series)\n",
    "    \n",
    "    return chunk_to_batches(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_x, valid_y = epoch(valid)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define the RNN\n",
    "\n",
    "Stack a few LSTM layers, then one more fully connected layer on top to the output. Hot-encode the input character and predict the next character in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Size of each lstm layer.\n",
    "lstm_layers = [128, 64, 64]\n",
    "\n",
    "# Ignore cost of first part of char series because the lstm memory is zero.\n",
    "ignore_front = 16\n",
    "\n",
    "# Reset batch size and series size so we accept any form of input.\n",
    "inputs_shape = [None, None] # [batch_size, series_size]\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prop\")\n",
    "\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        x = tf.placeholder(tf.int32, inputs_shape, name=\"x\")\n",
    "        hot_x = tf.one_hot(x, encoding_depth)\n",
    "\n",
    "    with tf.name_scope(\"targets\"):\n",
    "        y = tf.placeholder(tf.int32, inputs_shape, name=\"y\")\n",
    "        hot_y = tf.one_hot(y, encoding_depth)\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"RNN\"):\n",
    "        def drop_lstm(lstm_size):\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "            return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "        rnn = tf.contrib.rnn.MultiRNNCell([drop_lstm(s) for s in lstm_layers])\n",
    "\n",
    "        rnn_out, final_state = tf.nn.dynamic_rnn(rnn, hot_x, dtype=tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        weight = tf.Variable(tf.truncated_normal([lstm_layers[-1], encoding_depth], stddev=0.1), name=\"weight\")\n",
    "        bias = tf.Variable(tf.constant(0.0, shape=[encoding_depth]), name=\"bias\")\n",
    "\n",
    "        conn_out = tf.tensordot(rnn_out, weight, axes=[[2], [0]]) + bias\n",
    "\n",
    "    with tf.name_scope(\"output\"):\n",
    "        prediction = tf.nn.softmax(conn_out, dim=-1)\n",
    "\n",
    "        # Pick best char\n",
    "        predicted_y = tf.argmax(prediction, axis=-1)\n",
    "\n",
    "    with tf.name_scope(\"cost\"):\n",
    "        char_cross_entropy = -tf.reduce_sum(hot_y * tf.log(prediction), axis=-1)\n",
    "\n",
    "        batch_cross_entropy = tf.reduce_mean(char_cross_entropy[..., ignore_front:])\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(batch_cross_entropy)\n",
    "\n",
    "\n",
    "    # Summarize\n",
    "    tf.summary.histogram(\"expected_char\", y)\n",
    "\n",
    "    tf.summary.histogram(\"prediction\", prediction)\n",
    "    tf.summary.histogram('max_prediction', tf.reduce_max(prediction, axis=-1))\n",
    "\n",
    "    tf.summary.histogram(\"weight\", weight)\n",
    "    tf.summary.histogram(\"bias\", bias)\n",
    "\n",
    "    tf.summary.histogram(\"predicted_char\", predicted_y)\n",
    "    tf.summary.histogram(\"char_cross_entropy\", char_cross_entropy)\n",
    "\n",
    "    tf.summary.scalar(\"train_cross_entropy\", batch_cross_entropy)\n",
    "\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    validate_summary = tf.summary.scalar(\"validate_cross_entropy\", batch_cross_entropy)\n",
    "\n",
    "    # Details\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "run_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0\n",
      "  train_cross_entropy =    4.84508\n",
      "  valid_cross_entropy =    4.84666\n",
      "  \n",
      "  Expected : e, and you weigh this well;|Therefore still bear t\n",
      "  Predicted: \u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000U\u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000e\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "---\n",
      "Batch 200\n",
      "  train_cross_entropy =    3.22829\n",
      "  valid_cross_entropy =    3.31272\n",
      "  \n",
      "  Expected : herein thy counsel and consent is wanting.|Richard\n",
      "  Predicted:  \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "---\n",
      "Batch 400\n",
      "  train_cross_entropy =    3.26774\n",
      "  valid_cross_entropy =    3.30869\n",
      "  \n",
      "  Expected : ll me hence,|And therefore here I mean to take my \n",
      "  Predicted:  \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "---\n",
      "Batch 600\n",
      "  train_cross_entropy =    3.55027\n",
      "  valid_cross_entropy =    3.30336\n",
      "  \n",
      "  Expected : he power|So to seduce!--won to his shameful lust|T\n",
      "  Predicted:  \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "run_id += 1\n",
    "\n",
    "restore = None\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    if restore:\n",
    "        saver.restore(sess, restore)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./logs/{}\".format(run_id), sess.graph)\n",
    "\n",
    "    i = 0\n",
    "    for epoch in range(epochs):\n",
    "        for in_x, out_y in random_batched_epoch(train):\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={x: in_x, y: out_y, keep_prob: 0.9})\n",
    "\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                s, train_cost, p_y = sess.run([summary, batch_cross_entropy, predicted_y], feed_dict={\n",
    "                    x: in_x, y: out_y, keep_prob: 1.0})\n",
    "                \n",
    "                vs, valid_cost = sess.run([validate_summary, batch_cross_entropy], feed_dict={\n",
    "                    x: valid_x, y: valid_y, keep_prob: 1.0})\n",
    "\n",
    "                print(\"Batch {:>2}\".format(i))\n",
    "                print(\"  train_cross_entropy = {:>10.5f}\".format(train_cost))\n",
    "                print(\"  valid_cross_entropy = {:>10.5f}\".format(valid_cost))\n",
    "                print(\"  \")\n",
    "                print(\"  Expected : {}\".format(bytes(out_y[0][-50:]).decode(\"ascii\").replace(\"\\n\", \"|\")))\n",
    "                print(\"  Predicted: {}\".format(bytes(p_y[0][-50:]).decode(\"ascii\").replace(\"\\n\", \"|\")))\n",
    "                print(\"---\")\n",
    "                \n",
    "                writer.add_summary(s, i)\n",
    "                writer.add_summary(vs, i)\n",
    "                \n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            if i % 5000 == 0:\n",
    "                save_path = saver.save(sess, \"./saves/run_{}_batch_{}_epoch_{}.ckpt\".format(run_id, i, epoch))\n",
    "                print(\"\")\n",
    "                print(\"Model saved in file: {}.\".format(save_path))\n",
    "                print(\"===\")\n",
    "                \n",
    "                restore = save_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
